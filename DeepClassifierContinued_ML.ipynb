{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import sentencepiece\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#loading model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "#10 hours later...\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "#in first jupyter notebook: model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dataset = pd.read_csv('rand_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rand_dataset['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ideology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.502933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ideology\n",
       "count  86.000000\n",
       "mean    0.500000\n",
       "std     0.502933\n",
       "min     0.000000\n",
       "25%     0.000000\n",
       "50%     0.500000\n",
       "75%     1.000000\n",
       "max     1.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 32\n",
    "max_length = 9120\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "ideology = []\n",
    "for thing in rand_dataset.values:\n",
    "    speeches.append(thing[0])\n",
    "    ideology.append(thing[1])\n",
    "\n",
    "\n",
    "\n",
    "training_size = 70\n",
    "training_speeches = speeches[0:training_size]\n",
    "testing_speeches = speeches[training_size:]\n",
    "training_ideologies = ideology[0:training_size]\n",
    "testing_ideologies = ideology[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[0.46674457]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.52235556]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46737805]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.4890468]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5153033]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.47180894]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.50166976]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.49034956]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.49778813]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.47333908]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46786433]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46954525]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46614182]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46076554]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46753195]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.4761136]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.523831]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46526924]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.465584]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.49178466]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5086514]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.51066184]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.50956196]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.49425682]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.47956464]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.4655992]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5256023]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5235768]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5113399]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5230159]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.5090483]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.5077477]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.47758213]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.52385277]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.52210534]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.514751]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.4664477]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.5010162]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.46918887]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.49546155]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.49465472]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.47760347]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5247227]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5172978]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.48348343]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.4918564]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5026001]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.51603854]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.48939362]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.52397275]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.47980157]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.47929353]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.47204566]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.4663048]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.50722444]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.4680567]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.48002458]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46436712]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.51079303]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.5064016]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.5195668]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.49465588]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46398288]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.50466347]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46722776]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.46741614]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.47329405]]\n",
      "\n",
      "\n",
      "1\n",
      "[[0.50142425]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.460871]]\n",
      "\n",
      "\n",
      "0\n",
      "[[0.4721456]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_speeches)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_speeches)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_speeches)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "training_predictions = []\n",
    "\n",
    "for i in range(training_size):\n",
    "    sentence = [training_speeches[i]]\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    training_predictions.append(loaded_model.predict(padded))\n",
    "    print(training_ideologies[i])\n",
    "    print(loaded_model.predict(padded))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions_list = []\n",
    "for i in range(training_size):\n",
    "    training_predictions_list.append(training_predictions[i][0][0])\n",
    "#rand_dataset['predictions']= training_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "testing_predictions = []\n",
    "\n",
    "for i in range(16):\n",
    "    sentence = [testing_speeches[i]]\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    testing_predictions.append(loaded_model.predict(padded))\n",
    "    \n",
    "testing_predictions_list = []\n",
    "for i in range(16):\n",
    "    testing_predictions_list.append(testing_predictions[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "labels = np.array(training_ideologies)\n",
    "\n",
    "features = training_predictions_list\n",
    "features = np.array(features)\n",
    "\n",
    "test_features = testing_predictions_list\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(testing_ideologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.31 degrees.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "features = features.reshape(-1, 1)\n",
    "test_features = test_features.reshape(-1,1)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'random_forest_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
